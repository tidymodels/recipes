% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/woe.R
\name{step_woe}
\alias{step_woe}
\alias{tidy.step_woe}
\title{WoE Transformation}
\usage{
step_woe(recipe, ..., role = "predictor", outcome, trained = FALSE,
  dictionary = NULL, Laplace = 1e-06, prefix = "woe", skip = FALSE,
  id = rand_id("woe"))

\method{tidy}{step_woe}(x, ...)
}
\arguments{
\item{recipe}{A recipe object. The step will be added to the
sequence of operations for this recipe.}

\item{...}{One or more selector functions to choose which
variables will be used to compute the components. See
\code{\link[=selections]{selections()}} for more details. For the \code{tidy}
method, these are not currently used.}

\item{role}{For model terms created by this step, what analysis
role should they be assigned?. By default, the function assumes
that the new woe components columns created by the original
variables will be used as predictors in a model.}

\item{outcome}{bare name of the binary outcome.}

\item{trained}{A logical to indicate if the quantities for
preprocessing have been estimated.}

\item{dictionary}{a tbl. A map of levels and woe values. It must
have the same layout than the output returned from \code{\link[=dictionary]{dictionary()}}.
If `NULL`` the function will build a dictionary with those variables
passed to \code{...}. See \code{\link[=dictionary]{dictionary()}} for details.}

\item{Laplace}{value usually applied to avoid -Inf/Inf from predictor
category with only one outcome class. Set to 0 to allow Inf/-Inf.
The default is 1e-6. Also kwon as 'pseudocount' parameter of the
Laplace smoothing technique.}

\item{prefix}{A character string that will be the prefix to the
resulting new variables. See notes below}

\item{skip}{A logical. Should the step be skipped when the
recipe is baked by \code{\link[=bake.recipe]{bake.recipe()}}? While all operations are baked
when \code{\link[=prep.recipe]{prep.recipe()}} is run, some operations may not be able to be
conducted on new data (e.g. processing the outcome variable(s)).
Care should be taken when using \code{skip = TRUE} as it may affect
the computations for subsequent operations}

\item{id}{A character string that is unique to this step to identify it.}

\item{x}{A \code{step_woe} object.}
}
\value{
An updated version of \code{recipe} with the new step
added to the sequence of existing steps (if any). For the
\code{tidy} method, a tibble with the woe dictionary used to map
categories with woe values.
}
\description{
\code{step_woe} creates a \emph{specification} of a
recipe step that will transform nominal data into its numerical
transformation based on weights of evidence against a binary outcome.
}
\details{
WoE is a transformation of a group of variables that produces
a new set of features. The formula is

\deqn{woe_c = log((P(X = c|Y = 1))/(P(X = c|Y = 0)))}

where \eqn{c} goes from 1 to \eqn{C} levels of a given nominal
predictor variable \eqn{X}.

These components are designed to transform nominal variables into
numerical ones with the property that the order and magnitude
reflects the association with a binary outcome.  To apply it on
numerical predictors, it is advisable to discretize the variables
prior to running WoE. Here, each variable will be binarized to
have woe associated later. This can achieved by using \code{\link[=step_discretize]{step_discretize()}}.

The argument \code{Laplace} is an small quantity added to the
proportions of 1's and 0's with the goal to avoid log(p/0) or
log(0/p) results. The numerical woe versions will have names that
begin with \code{woe_} followed by the respecttive original name of the
variables. See Chen & Goodman (1996).

One can pass a custom \code{dictionary} tibble to \code{step_woe()}.
It must have the same structure of the output from
\code{dictionary()} (see examples). If not provided it will be
created automatically. The role of this tibble is to store the map
between the levels of nominal predictor to its woe values. You may
want to tweak this object with the goal to fix the orders between
the levels of one given predictor. One easy way to do this is by
tweaking an output returned from \code{dictionary()}.
}
\examples{

data("credit_data")

set.seed(111)
in_training <- sample(1:nrow(credit_data), 2000)

credit_tr <- credit_data[ in_training, ]
credit_te <- credit_data[-in_training, ]

rec <- recipe(Status ~ ., data = credit_tr) \%>\%
  step_woe(Job, Home, outcome = Status)

woe_models <- prep(rec, training = credit_tr)

woe_te <- bake(woe_models, new_data = credit_te)

head(woe_te)
tidy(rec, number = 1)
tidy(woe_models, number = 1)

# Example of custom dictionary + tweaking
# custom dictionary
woe_dict_custom <- credit_tr \%>\% dictionary(Job, Home, outcome = Status)
woe_dict_custom[4, "woe"] <- 1.23 #tweak

#passing custom dict to step_woe()
rec_custom <- recipe(Status ~ ., data = credit_tr) \%>\%
  step_woe(Job, Home, outcome = Status, dictionary = woe_dict_custom) \%>\%
  prep

rec_custom_baked <- bake(rec_custom, new_data = credit_te)
rec_custom_baked \%>\% dplyr::filter(woe_Job == 1.23) \%>\% head

}
\references{
Kullback, S. (1959). \emph{Information Theory and Statistics.} Wiley, New York.

Hastie, T., Tibshirani, R. and Friedman, J. (1986). \emph{Elements of Statistical Learning}, Second Edition, Springer, 2009.

SF Chen, J Goodman (1996). \emph{An empirical study of smoothing techniques for language modeling.} Proceedings of the 34th annual meeting on Association for Computational Linguistics.
}
\concept{preprocessing woe transformation_methods}
\keyword{datagen}
